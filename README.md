# üõ†Ô∏è Data Engineering Prep for Big tech

This project is designed to strengthen and demonstrate data engineering skills through hands-on implementation of key concepts including ETL/ELT, data warehousing, batch & streaming pipelines, and cloud-based data workflows.
Also including DSA, SQL and System Design

---

## üìå Objectives

- Leetcode
- Build scalable ETL/ELT pipelines
- Practice SQL and data modeling techniques
- Integrate batch and real-time data processing
- Automate workflows using orchestration tools
- Visualize data using BI tools

---

## üß∞ Tech Stack

| Category              | Tools / Technologies                             |
|-----------------------|--------------------------------------------------|
| Programming           | Python, SQL                                      |
| Orchestration         | Apache Airflow                     |
| Batch Processing      | Pandas, PySpark                                  |
| Streaming             | Kafka        |
| Cloud Platform        | Azure / AWS / GCP          |
| Data Warehousing      | BigQuery / Snowflake / Azure Synapse             |
| BI & Visualization    | Power BI / Looker / Streamlit                    |
| CI/CD & Automation    | GitHub Actions / dbt                             |
| Data Storage          | Azure Blob Storage / Amazon S3                   |

---

## üìà Features

-  **Batch Pipeline**: CSV/JSON to Cloud Warehouse via Data Factory / custom ETL
-  **Streaming Pipeline**: Real-time ingestion using Kafka or Event Hub
-  **Data Modeling**: Star schema for analytics-ready data
-  **Reporting**: Dashboards built using Power BI and Streamlit
-  **Automation**: Scheduled DAGs with Airflow or Prefect
-  **Testing**: Unit tests to validate transformations and logic

## by Selenge
